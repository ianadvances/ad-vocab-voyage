"""
è©å½™ç”Ÿæˆå™¨ç¯„ä¾‹ç¨‹å¼

é€™å€‹ç¨‹å¼å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ AI æ¨¡å‹ï¼ˆGoogle Geminiï¼‰è‡ªå‹•ç”Ÿæˆç‰¹å®šä¸»é¡Œçš„è‹±èªè©å½™è¡¨ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. é‡å°ä¸åŒä¸»é¡Œç”Ÿæˆç›¸é—œçš„è‹±èªè©å½™
2. æä¾›ç¹é«”ä¸­æ–‡ç¿»è­¯å’Œè©æ€§æ¨™è¨»
3. é¿å…é‡è¤‡è©å½™çš„ç”Ÿæˆ
4. å°‡ç”Ÿæˆçš„è©å½™ä¿å­˜åˆ°æ–‡ä»¶ä¸­

æŠ€è¡“ç‰¹é»ï¼š
- ä½¿ç”¨ Google Vertex AI çš„ Gemini æ¨¡å‹é€²è¡Œè©å½™ç”Ÿæˆ
- æ”¯æ´å¤šå€‹ä¸»é¡Œçš„è©å½™ç”Ÿæˆ
- è‡ªå‹•æª¢æ¸¬å’Œé¿å…é‡è¤‡è©å½™
- å°‡çµæœä¿å­˜ç‚ºçµæ§‹åŒ–çš„æ–‡æœ¬æ–‡ä»¶

ä½¿ç”¨å ´æ™¯ï¼š
- è‹±èªå­¸ç¿’æ‡‰ç”¨çš„è©å½™åº«å»ºç«‹
- ä¸»é¡Œå¼è©å½™è¡¨çš„è‡ªå‹•ç”Ÿæˆ
- èªè¨€å­¸ç¿’è³‡æºçš„æ‰¹é‡å‰µå»º

ä½œè€…ï¼šVocabVoyage åœ˜éšŠ
æ—¥æœŸï¼š2024å¹´
"""

import time
import json
from vertexai.generative_models import GenerativeModel
from pathlib import Path
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ============================================================================
# é…ç½®å’Œå¸¸æ•¸å®šç¾©
# ============================================================================

# å®šç¾©è¦ç”Ÿæˆè©å½™çš„ä¸»é¡Œåˆ—è¡¨
# å¯ä»¥æ ¹æ“šéœ€è¦æ·»åŠ æˆ–ä¿®æ”¹ä¸»é¡Œ
TOPICS = [
    "Daily Life",              # æ—¥å¸¸ç”Ÿæ´»
    "Work & Career",           # å·¥ä½œèˆ‡è·æ¥­
    "Travel & Transportation", # æ—…è¡Œèˆ‡äº¤é€š
    "Food & Dining",          # é£Ÿç‰©èˆ‡ç”¨é¤
    "Education & Learning",    # æ•™è‚²èˆ‡å­¸ç¿’
    "Technology & Digital",    # ç§‘æŠ€èˆ‡æ•¸ä½
    "Health & Medical",       # å¥åº·èˆ‡é†«ç™‚
    "Entertainment & Leisure", # å¨›æ¨‚èˆ‡ä¼‘é–’
    "Environment & Nature",    # ç’°å¢ƒèˆ‡è‡ªç„¶
    "Social Relationships"     # ç¤¾äº¤é—œä¿‚
]

# æ¯å€‹ä¸»é¡Œè¦ç”Ÿæˆçš„è©å½™æ•¸é‡
WORDS_PER_TOPIC = 150

# API è«‹æ±‚ä¹‹é–“çš„å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰
REQUEST_DELAY = 1

# ============================================================================
# å…¨åŸŸè®Šæ•¸
# ============================================================================

# ç”¨æ–¼å„²å­˜æ‰€æœ‰å·²ç”Ÿæˆçš„å–®å­—ï¼Œé¿å…é‡è¤‡
generated_words = set()

# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸
# ============================================================================

def generate_vocabulary(topic: str) -> str:
    """
    ç‚ºæŒ‡å®šä¸»é¡Œç”Ÿæˆè©å½™è¡¨
    
    ä½¿ç”¨ Google Gemini æ¨¡å‹ç”Ÿæˆç‰¹å®šä¸»é¡Œçš„è‹±èªè©å½™ï¼ŒåŒ…å«ç¹é«”ä¸­æ–‡ç¿»è­¯ã€‚
    
    Args:
        topic (str): è¦ç”Ÿæˆè©å½™çš„ä¸»é¡Œåç¨±
        
    Returns:
        str: ç”Ÿæˆçš„è©å½™å…§å®¹ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
        
    Raises:
        Exception: ç•¶ API èª¿ç”¨å¤±æ•—æ™‚æ‹‹å‡ºç•°å¸¸
    """
    try:
        # å»ºç«‹æ‰€æœ‰ä¸»é¡Œçš„å­—ä¸²ï¼Œä¸¦æ¨™è¨˜ç•¶å‰æ­£åœ¨è™•ç†çš„ä¸»é¡Œ
        all_topics_str = "\n".join([
            f"{'-> ' if t == topic else '   '}{t}" 
            for t in TOPICS
        ])
        
        # å»ºç«‹è©³ç´°çš„æç¤ºè©ï¼ŒæŒ‡å° AI ç”Ÿæˆé«˜å“è³ªçš„è©å½™
        prompt = f"""æˆ‘æ­£åœ¨ç‚ºä»¥ä¸‹ä¸»é¡Œå‰µå»ºè©å½™è¡¨ï¼š

{all_topics_str}

ç›®å‰æ­£åœ¨ç”Ÿæˆä¸»é¡Œï¼š{topic}

è«‹ç‚º {topic} ç”Ÿæˆ {WORDS_PER_TOPIC} å€‹ä¸­é«˜ç´šç¨‹åº¦çš„è‹±èªå–®å­—æˆ–ç‰‡èªã€‚è¦æ±‚ï¼š

1. æ ¼å¼ï¼šæ¯è¡Œä¸€å€‹è©å½™ï¼Œæ ¼å¼ç‚ºã€Œè‹±æ–‡å–®å­—/ç‰‡èª - ç¹é«”ä¸­æ–‡ç¿»è­¯ (è©æ€§)ã€
2. è©å½™æ‡‰è©²å¯¦ç”¨ä¸”å¸¸ç”¨
3. åŒ…å«ä¸åŒè©æ€§ï¼ˆåè©ã€å‹•è©ã€å½¢å®¹è©ã€ç‰‡èªç­‰ï¼‰
4. ç¢ºä¿æ¯å€‹å–®å­—/ç‰‡èªï¼š
   - èˆ‡ç•¶å‰ä¸»é¡Œã€Œ{topic}ã€é«˜åº¦ç›¸é—œ
   - ä¸æœƒå¤ªé€šç”¨ä»¥è‡³æ–¼å¯èƒ½å‡ºç¾åœ¨å…¶ä»–ä¸»é¡Œä¸­
   - åœ¨æ­¤ä¸»é¡Œå…§æ˜¯å”¯ä¸€çš„ï¼Œä¸é‡è¤‡
5. å°æ–¼å¯ä»¥ä½œç‚ºä¸åŒè©æ€§çš„å–®å­—ï¼Œè«‹æ¨™è¨»æ‰€æœ‰å¯èƒ½çš„ç”¨æ³•
6. æ‰€æœ‰ä¸­æ–‡ç¿»è­¯å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡å­—
7. åªæä¾›è©å½™åˆ—è¡¨ï¼Œä¸éœ€è¦é¡å¤–çš„è§£é‡‹æˆ–è¨»é‡‹

ç¯„ä¾‹æ ¼å¼ï¼š
1. advocate - æå€¡ï¼Œå€¡å° (v.) / æ“è­·è€…ï¼Œæå€¡è€… (n.)
2. sustainable - å¯æŒçºŒçš„ï¼Œæ°¸çºŒçš„ (adj.)
3. ...

è«‹é–‹å§‹ç”Ÿæˆï¼š"""

        # åˆå§‹åŒ– Gemini æ¨¡å‹
        model = GenerativeModel('gemini-1.5-pro-001')
        
        # ç™¼é€è«‹æ±‚ä¸¦ç²å–å›æ‡‰
        print(f"  æ­£åœ¨å‘ Gemini API ç™¼é€è«‹æ±‚...")
        response = model.generate_content(prompt)
        content = response.text
        
        # æå–æ–°ç”Ÿæˆçš„å–®å­—ï¼ˆå–è‹±æ–‡éƒ¨åˆ†é€²è¡Œé‡è¤‡æª¢æŸ¥ï¼‰
        new_words = []
        for line in content.split('\n'):
            if '-' in line:
                # æå–è‹±æ–‡éƒ¨åˆ†ä¸¦è½‰ç‚ºå°å¯«é€²è¡Œæ¯”è¼ƒ
                english_part = line.split('-')[0].strip().lower()
                # ç§»é™¤è¡Œè™Ÿï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                english_part = english_part.split('.', 1)[-1].strip()
                new_words.append(english_part)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„å–®å­—
        duplicates = set(new_words) & generated_words
        if duplicates:
            print(f"  âš ï¸  è­¦å‘Šï¼šåœ¨ {topic} ä¸­ç™¼ç¾é‡è¤‡å–®å­—ï¼š{duplicates}")
        
        # æ›´æ–°å…¨åŸŸå–®å­—é›†åˆ
        generated_words.update(new_words)
        
        print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(new_words)} å€‹è©å½™")
        return content
        
    except Exception as e:
        print(f"  âŒ ç”Ÿæˆ {topic} è©å½™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return None

def save_to_file(topic: str, content: str) -> bool:
    """
    å°‡ç”Ÿæˆçš„è©å½™å…§å®¹ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        topic (str): ä¸»é¡Œåç¨±
        content (str): è¦ä¿å­˜çš„è©å½™å…§å®¹
        
    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å› Trueï¼Œå¤±æ•—è¿”å› False
    """
    try:
        # å‰µå»ºè¼¸å‡ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        output_dir = Path("data/vocabulary")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å°‡ä¸»é¡Œåç¨±è½‰æ›ç‚ºé©åˆçš„æª”æ¡ˆåç¨±
        # ä¾‹å¦‚ï¼š"Work & Career" -> "work_career.txt"
        filename = topic.lower().replace(" & ", "_").replace(" ", "_") + ".txt"
        file_path = output_dir / filename
        
        # å¯«å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"ä¸»é¡Œï¼š{topic}\n")
            f.write("=" * 50 + "\n\n")
            f.write(content)
        
        print(f"  ğŸ’¾ æˆåŠŸä¿å­˜è©å½™åˆ°ï¼š{file_path}")
        
        # ä¿å­˜å·²ç”Ÿæˆçš„å–®å­—åˆ—è¡¨ï¼ˆç”¨æ–¼é™¤éŒ¯å’Œçµ±è¨ˆï¼‰
        debug_file = output_dir / "generated_words_list.json"
        with open(debug_file, "w", encoding="utf-8") as f:
            json.dump(
                list(generated_words), 
                f, 
                ensure_ascii=False, 
                indent=2,
                sort_keys=True
            )
        
        return True
        
    except Exception as e:
        print(f"  âŒ ä¿å­˜ {topic} æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return False

def generate_statistics() -> dict:
    """
    ç”Ÿæˆè©å½™ç”Ÿæˆçš„çµ±è¨ˆè³‡è¨Š
    
    Returns:
        dict: åŒ…å«çµ±è¨ˆè³‡è¨Šçš„å­—å…¸
    """
    return {
        "total_topics": len(TOPICS),
        "total_unique_words": len(generated_words),
        "average_words_per_topic": len(generated_words) / len(TOPICS) if TOPICS else 0,
        "target_words_per_topic": WORDS_PER_TOPIC
    }

def save_statistics(stats: dict):
    """
    ä¿å­˜çµ±è¨ˆè³‡è¨Šåˆ°æ–‡ä»¶
    
    Args:
        stats (dict): çµ±è¨ˆè³‡è¨Šå­—å…¸
    """
    try:
        output_dir = Path("data/vocabulary")
        stats_file = output_dir / "generation_statistics.json"
        
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š çµ±è¨ˆè³‡è¨Šå·²ä¿å­˜åˆ°ï¼š{stats_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜çµ±è¨ˆè³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

# ============================================================================
# ä¸»ç¨‹å¼é‚è¼¯
# ============================================================================

def main():
    """
    ä¸»ç¨‹å¼å‡½æ•¸
    
    åŸ·è¡Œå®Œæ•´çš„è©å½™ç”Ÿæˆæµç¨‹ï¼š
    1. éæ­·æ‰€æœ‰ä¸»é¡Œ
    2. ç‚ºæ¯å€‹ä¸»é¡Œç”Ÿæˆè©å½™
    3. ä¿å­˜ç”Ÿæˆçš„å…§å®¹
    4. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
    """
    print("ğŸš€ é–‹å§‹è©å½™ç”Ÿæˆç¨‹åº")
    print("=" * 60)
    
    successful_generations = 0
    failed_generations = 0
    
    # éæ­·æ‰€æœ‰ä¸»é¡Œé€²è¡Œè©å½™ç”Ÿæˆ
    for i, topic in enumerate(TOPICS, 1):
        print(f"\nğŸ“š [{i}/{len(TOPICS)}] æ­£åœ¨è™•ç†ä¸»é¡Œï¼š{topic}")
        print("-" * 40)
        
        # ç”Ÿæˆè©å½™å…§å®¹
        content = generate_vocabulary(topic)
        
        if content:
            # ä¿å­˜åˆ°æ–‡ä»¶
            if save_to_file(topic, content):
                successful_generations += 1
            else:
                failed_generations += 1
                
            # åœ¨æ¯æ¬¡è«‹æ±‚ä¹‹é–“æš«åœï¼Œé¿å…è¶…é API é€Ÿç‡é™åˆ¶
            if i < len(TOPICS):  # æœ€å¾Œä¸€å€‹ä¸»é¡Œä¸éœ€è¦ç­‰å¾…
                print(f"  â³ ç­‰å¾… {REQUEST_DELAY} ç§’å¾Œç¹¼çºŒ...")
                time.sleep(REQUEST_DELAY)
        else:
            print(f"  â­ï¸  è·³é {topic}ï¼Œå› ç‚ºç”Ÿæˆå¤±æ•—")
            failed_generations += 1
        
        # é¡¯ç¤ºç›®å‰é€²åº¦
        print(f"  ğŸ“ˆ ç›®å‰å·²ç”Ÿæˆçš„å”¯ä¸€è©å½™ç¸½æ•¸ï¼š{len(generated_words)}")
    
    # ç”Ÿæˆæœ€çµ‚çµ±è¨ˆå ±å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š è©å½™ç”Ÿæˆå®Œæˆçµ±è¨ˆ")
    print("=" * 60)
    
    stats = generate_statistics()
    
    print(f"âœ… æˆåŠŸç”Ÿæˆçš„ä¸»é¡Œï¼š{successful_generations}")
    print(f"âŒ å¤±æ•—çš„ä¸»é¡Œï¼š{failed_generations}")
    print(f"ğŸ“ ç¸½ä¸»é¡Œæ•¸ï¼š{stats['total_topics']}")
    print(f"ğŸ”¤ å”¯ä¸€è©å½™ç¸½æ•¸ï¼š{stats['total_unique_words']}")
    print(f"ğŸ“Š å¹³å‡æ¯ä¸»é¡Œè©å½™æ•¸ï¼š{stats['average_words_per_topic']:.1f}")
    print(f"ğŸ¯ ç›®æ¨™æ¯ä¸»é¡Œè©å½™æ•¸ï¼š{stats['target_words_per_topic']}")
    
    # ä¿å­˜çµ±è¨ˆè³‡è¨Š
    save_statistics(stats)
    
    if successful_generations == len(TOPICS):
        print("\nğŸ‰ æ‰€æœ‰ä¸»é¡Œçš„è©å½™ç”Ÿæˆå®Œæˆï¼")
    else:
        print(f"\nâš ï¸  æœ‰ {failed_generations} å€‹ä¸»é¡Œç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")

# ============================================================================
# è¼”åŠ©åŠŸèƒ½å‡½æ•¸
# ============================================================================

def preview_topic_list():
    """é è¦½å°‡è¦è™•ç†çš„ä¸»é¡Œåˆ—è¡¨"""
    print("ğŸ“‹ å°‡è¦ç”Ÿæˆè©å½™çš„ä¸»é¡Œåˆ—è¡¨ï¼š")
    print("-" * 30)
    for i, topic in enumerate(TOPICS, 1):
        print(f"{i:2d}. {topic}")
    print(f"\nç¸½å…± {len(TOPICS)} å€‹ä¸»é¡Œ")

def estimate_time():
    """ä¼°ç®—å®Œæˆæ™‚é–“"""
    total_time = len(TOPICS) * REQUEST_DELAY
    minutes = total_time // 60
    seconds = total_time % 60
    print(f"â±ï¸  é ä¼°å®Œæˆæ™‚é–“ï¼šç´„ {minutes} åˆ† {seconds} ç§’")

# ============================================================================
# ç¨‹å¼å…¥å£é»
# ============================================================================

if __name__ == "__main__":
    import sys
    
    print("VocabVoyage è©å½™ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        if sys.argv[1] == "--preview":
            preview_topic_list()
            estimate_time()
            sys.exit(0)
        elif sys.argv[1] == "--help":
            print("ä½¿ç”¨æ–¹æ³•ï¼š")
            print("  python vocabulary_generator.py          # é–‹å§‹ç”Ÿæˆè©å½™")
            print("  python vocabulary_generator.py --preview # é è¦½ä¸»é¡Œåˆ—è¡¨")
            print("  python vocabulary_generator.py --help    # é¡¯ç¤ºå¹«åŠ©")
            sys.exit(0)
    
    # é¡¯ç¤ºé è¦½è³‡è¨Š
    preview_topic_list()
    estimate_time()
    
    # ç¢ºèªæ˜¯å¦ç¹¼çºŒ
    try:
        response = input("\næ˜¯å¦é–‹å§‹ç”Ÿæˆè©å½™ï¼Ÿ(y/N): ").strip().lower()
        if response not in ['y', 'yes', 'æ˜¯']:
            print("å·²å–æ¶ˆè©å½™ç”Ÿæˆ")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\n\nå·²å–æ¶ˆè©å½™ç”Ÿæˆ")
        sys.exit(0)
    
    # åŸ·è¡Œä¸»ç¨‹å¼
    main()