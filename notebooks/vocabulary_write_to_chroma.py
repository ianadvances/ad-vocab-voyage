"""
è©å½™å‘é‡è³‡æ–™åº«å¯«å…¥ç¯„ä¾‹ç¨‹å¼

é€™å€‹ç¨‹å¼å±•ç¤ºäº†å¦‚ä½•å°‡ç”Ÿæˆçš„è©å½™æ–‡ä»¶è¼‰å…¥åˆ° Chroma å‘é‡è³‡æ–™åº«ä¸­ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è®€å– data/vocabulary ç›®éŒ„ä¸‹çš„æ‰€æœ‰è©å½™æ–‡ä»¶
2. å°‡è©å½™å…§å®¹è½‰æ›ç‚ºå‘é‡åµŒå…¥
3. å­˜å„²åˆ° Chroma å‘é‡è³‡æ–™åº«ä¸­
4. æ”¯æ´åŸºæ–¼ä¸»é¡Œçš„å…ƒè³‡æ–™æ¨™è¨˜

æŠ€è¡“ç‰¹é»ï¼š
- ä½¿ç”¨ OpenAI çš„ text-embedding-3-small æ¨¡å‹ç”Ÿæˆå‘é‡åµŒå…¥
- æ”¯æ´æ‰¹é‡è™•ç†å¤šå€‹è©å½™æ–‡ä»¶
- è‡ªå‹•æå–ä¸»é¡Œè³‡è¨Šä½œç‚ºå…ƒè³‡æ–™
- æŒä¹…åŒ–å­˜å„²åˆ°æœ¬åœ° Chroma è³‡æ–™åº«

ä½¿ç”¨å ´æ™¯ï¼š
- å»ºç«‹è©å½™æª¢ç´¢ç³»çµ±
- æ”¯æ´èªç¾©æœç´¢åŠŸèƒ½
- ç‚º RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ) ç³»çµ±æä¾›è©å½™çŸ¥è­˜åº«
- ä¸»é¡Œå¼è©å½™åˆ†é¡å’Œæª¢ç´¢

ä½œè€…ï¼šVocabVoyage åœ˜éšŠ
æ—¥æœŸï¼š2024å¹´
"""

from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
import os
import json
from pathlib import Path
from typing import List, Dict, Optional
import time

# ============================================================================
# é…ç½®å’Œå¸¸æ•¸
# ============================================================================

# è©å½™æ–‡ä»¶ç›®éŒ„
VOCABULARY_DIR = "data/vocabulary"

# Chroma è³‡æ–™åº«ç›®éŒ„
CHROMA_DB_DIR = "./data/chroma_db"

# Chroma é›†åˆåç¨±
COLLECTION_NAME = "vocabulary_v1"

# OpenAI åµŒå…¥æ¨¡å‹
EMBEDDING_MODEL = "text-embedding-3-small"

# æ”¯æ´çš„æ–‡ä»¶å‰¯æª”å
SUPPORTED_EXTENSIONS = ['.txt']

# ============================================================================
# è©å½™æ–‡ä»¶è¼‰å…¥åŠŸèƒ½
# ============================================================================

def load_vocabulary_files(vocab_dir: str = VOCABULARY_DIR) -> List[Document]:
    """
    è¼‰å…¥è©å½™ç›®éŒ„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ä¸¦è½‰æ›ç‚º Document å°è±¡
    
    é€™å€‹å‡½æ•¸æœƒéæ­·æŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰ .txt æ–‡ä»¶ï¼Œè®€å–å…§å®¹ä¸¦å‰µå»º
    LangChain Document å°è±¡ï¼ŒåŒæ™‚æå–ä¸»é¡Œè³‡è¨Šä½œç‚ºå…ƒè³‡æ–™ã€‚
    
    Args:
        vocab_dir (str): è©å½™æ–‡ä»¶ç›®éŒ„è·¯å¾‘
        
    Returns:
        List[Document]: Document å°è±¡åˆ—è¡¨
        
    Raises:
        FileNotFoundError: ç•¶è©å½™ç›®éŒ„ä¸å­˜åœ¨æ™‚
        Exception: ç•¶è®€å–æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤
    """
    try:
        print(f"ğŸ“‚ é–‹å§‹è¼‰å…¥è©å½™æ–‡ä»¶ï¼š{vocab_dir}")
        
        # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
        vocab_path = Path(vocab_dir)
        if not vocab_path.exists():
            raise FileNotFoundError(f"è©å½™ç›®éŒ„ä¸å­˜åœ¨ï¼š{vocab_dir}")
        
        documents = []
        processed_files = 0
        skipped_files = 0
        
        # éæ­·ç›®éŒ„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        for file_path in vocab_path.iterdir():
            if file_path.is_file() and file_path.suffix in SUPPORTED_EXTENSIONS:
                try:
                    print(f"  ğŸ“„ æ­£åœ¨è™•ç†ï¼š{file_path.name}")
                    
                    # è®€å–æ–‡ä»¶å…§å®¹
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç‚ºç©º
                    if not content.strip():
                        print(f"    âš ï¸  è·³éç©ºæ–‡ä»¶ï¼š{file_path.name}")
                        skipped_files += 1
                        continue
                    
                    # å¾æ–‡ä»¶åä¸­æå–ä¸»é¡Œ
                    topic = extract_topic_from_filename(file_path.name)
                    
                    # å¾æ–‡ä»¶å…§å®¹ä¸­æå–é¡å¤–çš„å…ƒè³‡æ–™
                    metadata = extract_metadata_from_content(content, topic)
                    
                    # å‰µå»º Document å°è±¡
                    doc = Document(
                        page_content=content,
                        metadata=metadata
                    )
                    
                    documents.append(doc)
                    processed_files += 1
                    
                    print(f"    âœ… æˆåŠŸè¼‰å…¥ï¼Œå…§å®¹é•·åº¦ï¼š{len(content)} å­—ç¬¦")
                    
                except Exception as e:
                    print(f"    âŒ è¼‰å…¥æ–‡ä»¶å¤±æ•—ï¼š{file_path.name} - {str(e)}")
                    skipped_files += 1
                    continue
            else:
                # è·³éä¸æ”¯æ´çš„æ–‡ä»¶é¡å‹
                if file_path.is_file():
                    print(f"  â­ï¸  è·³éä¸æ”¯æ´çš„æ–‡ä»¶ï¼š{file_path.name}")
                    skipped_files += 1
        
        print(f"\nğŸ“Š è¼‰å…¥çµ±è¨ˆï¼š")
        print(f"  âœ… æˆåŠŸè¼‰å…¥ï¼š{processed_files} å€‹æ–‡ä»¶")
        print(f"  â­ï¸  è·³éæ–‡ä»¶ï¼š{skipped_files} å€‹æ–‡ä»¶")
        print(f"  ğŸ“ ç¸½æ–‡æª”æ•¸ï¼š{len(documents)} å€‹")
        
        return documents
        
    except FileNotFoundError as e:
        print(f"âŒ ç›®éŒ„éŒ¯èª¤ï¼š{e}")
        return []
    except Exception as e:
        print(f"âŒ è¼‰å…¥è©å½™æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return []

def extract_topic_from_filename(filename: str) -> str:
    """
    å¾æ–‡ä»¶åä¸­æå–ä¸»é¡Œåç¨±
    
    Args:
        filename (str): æ–‡ä»¶å
        
    Returns:
        str: æå–çš„ä¸»é¡Œåç¨±
    """
    # ç§»é™¤å‰¯æª”å
    topic = Path(filename).stem
    
    # å°‡åº•ç·šè½‰æ›ç‚ºç©ºæ ¼ä¸¦è½‰æ›ç‚ºæ¨™é¡Œæ ¼å¼
    topic = topic.replace('_', ' ').title()
    
    return topic

def extract_metadata_from_content(content: str, topic: str) -> Dict[str, str]:
    """
    å¾æ–‡ä»¶å…§å®¹ä¸­æå–å…ƒè³‡æ–™
    
    Args:
        content (str): æ–‡ä»¶å…§å®¹
        topic (str): ä¸»é¡Œåç¨±
        
    Returns:
        Dict[str, str]: å…ƒè³‡æ–™å­—å…¸
    """
    metadata = {
        "topic": topic,
        "content_type": "vocabulary",
        "language": "en-zh",  # è‹±æ–‡-ä¸­æ–‡
    }
    
    # çµ±è¨ˆè©å½™æ•¸é‡ï¼ˆç°¡å–®è¨ˆç®—è¡Œæ•¸ï¼‰
    lines = [line.strip() for line in content.split('\n') if line.strip()]
    vocab_lines = [line for line in lines if '-' in line]  # åŒ…å«ç¿»è­¯çš„è¡Œ
    
    metadata["estimated_vocab_count"] = str(len(vocab_lines))
    metadata["total_lines"] = str(len(lines))
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸»é¡Œæ¨™è¨˜
    if "ä¸»é¡Œï¼š" in content or "Topic:" in content:
        metadata["has_topic_header"] = "true"
    else:
        metadata["has_topic_header"] = "false"
    
    return metadata

# ============================================================================
# å‘é‡è³‡æ–™åº«æ“ä½œåŠŸèƒ½
# ============================================================================

def create_vector_store(documents: List[Document], 
                       collection_name: str = COLLECTION_NAME,
                       persist_dir: str = CHROMA_DB_DIR) -> Optional[Chroma]:
    """
    å‰µå»º Chroma å‘é‡è³‡æ–™åº«ä¸¦å­˜å„²æ–‡æª”
    
    Args:
        documents (List[Document]): è¦å­˜å„²çš„æ–‡æª”åˆ—è¡¨
        collection_name (str): é›†åˆåç¨±
        persist_dir (str): æŒä¹…åŒ–ç›®éŒ„
        
    Returns:
        Optional[Chroma]: å‰µå»ºçš„å‘é‡å­˜å„²å°è±¡ï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    try:
        print(f"ğŸ”„ é–‹å§‹å‰µå»ºå‘é‡è³‡æ–™åº«...")
        print(f"  ğŸ“ é›†åˆåç¨±ï¼š{collection_name}")
        print(f"  ğŸ“ å­˜å„²ç›®éŒ„ï¼š{persist_dir}")
        print(f"  ğŸ¤– åµŒå…¥æ¨¡å‹ï¼š{EMBEDDING_MODEL}")
        
        # ç¢ºä¿å­˜å„²ç›®éŒ„å­˜åœ¨
        Path(persist_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("  ğŸ”§ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
        
        # å‰µå»ºå‘é‡å­˜å„²
        print(f"  ğŸ“Š é–‹å§‹è™•ç† {len(documents)} å€‹æ–‡æª”...")
        start_time = time.time()
        
        vectorstore = Chroma.from_documents(
            documents=documents,
            collection_name=collection_name,
            embedding=embeddings,
            persist_directory=persist_dir
        )
        
        processing_time = time.time() - start_time
        print(f"  âœ… å‘é‡è³‡æ–™åº«å‰µå»ºå®Œæˆï¼Œè€—æ™‚ï¼š{processing_time:.2f} ç§’")
        
        return vectorstore
        
    except Exception as e:
        print(f"âŒ å‰µå»ºå‘é‡è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return None

def verify_vector_store(vectorstore: Chroma) -> bool:
    """
    é©—è­‰å‘é‡è³‡æ–™åº«çš„å®Œæ•´æ€§
    
    Args:
        vectorstore (Chroma): å‘é‡å­˜å„²å°è±¡
        
    Returns:
        bool: é©—è­‰æˆåŠŸè¿”å› Trueï¼Œå¤±æ•—è¿”å› False
    """
    try:
        print("ğŸ” é©—è­‰å‘é‡è³‡æ–™åº«...")
        
        # ç²å–é›†åˆè³‡è¨Š
        collection = vectorstore._collection
        doc_count = collection.count()
        
        print(f"  ğŸ“Š æ–‡æª”ç¸½æ•¸ï¼š{doc_count}")
        
        # æ¸¬è©¦æª¢ç´¢åŠŸèƒ½
        print("  ğŸ” æ¸¬è©¦æª¢ç´¢åŠŸèƒ½...")
        test_query = "vocabulary learning"
        results = vectorstore.similarity_search(test_query, k=3)
        
        print(f"  ğŸ“ æ¸¬è©¦æŸ¥è©¢ï¼š'{test_query}'")
        print(f"  ğŸ“‹ è¿”å›çµæœï¼š{len(results)} å€‹æ–‡æª”")
        
        if results:
            print("  âœ… æª¢ç´¢åŠŸèƒ½æ­£å¸¸")
            
            # é¡¯ç¤ºç¬¬ä¸€å€‹çµæœçš„å…ƒè³‡æ–™
            first_result = results[0]
            print(f"  ğŸ“„ ç¬¬ä¸€å€‹çµæœä¸»é¡Œï¼š{first_result.metadata.get('topic', 'Unknown')}")
            
            return True
        else:
            print("  âš ï¸  æª¢ç´¢æœªè¿”å›çµæœ")
            return False
            
    except Exception as e:
        print(f"âŒ é©—è­‰å‘é‡è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return False

# ============================================================================
# çµ±è¨ˆå’Œå ±å‘ŠåŠŸèƒ½
# ============================================================================

def generate_statistics(documents: List[Document]) -> Dict:
    """
    ç”Ÿæˆè©å½™è™•ç†çµ±è¨ˆè³‡è¨Š
    
    Args:
        documents (List[Document]): è™•ç†çš„æ–‡æª”åˆ—è¡¨
        
    Returns:
        Dict: çµ±è¨ˆè³‡è¨Šå­—å…¸
    """
    stats = {
        "total_documents": len(documents),
        "topics": {},
        "total_content_length": 0,
        "estimated_total_vocab": 0
    }
    
    for doc in documents:
        topic = doc.metadata.get("topic", "Unknown")
        content_length = len(doc.page_content)
        vocab_count = int(doc.metadata.get("estimated_vocab_count", "0"))
        
        # çµ±è¨ˆä¸»é¡Œè³‡è¨Š
        if topic not in stats["topics"]:
            stats["topics"][topic] = {
                "document_count": 0,
                "content_length": 0,
                "vocab_count": 0
            }
        
        stats["topics"][topic]["document_count"] += 1
        stats["topics"][topic]["content_length"] += content_length
        stats["topics"][topic]["vocab_count"] += vocab_count
        
        # ç¸½è¨ˆ
        stats["total_content_length"] += content_length
        stats["estimated_total_vocab"] += vocab_count
    
    return stats

def print_statistics(stats: Dict):
    """
    æ‰“å°çµ±è¨ˆè³‡è¨Š
    
    Args:
        stats (Dict): çµ±è¨ˆè³‡è¨Šå­—å…¸
    """
    print("\nğŸ“Š è©å½™è™•ç†çµ±è¨ˆå ±å‘Š")
    print("=" * 60)
    
    print(f"ğŸ“ ç¸½æ–‡æª”æ•¸ï¼š{stats['total_documents']}")
    print(f"ğŸ”¤ ä¼°è¨ˆè©å½™ç¸½æ•¸ï¼š{stats['estimated_total_vocab']:,}")
    print(f"ğŸ“„ ç¸½å…§å®¹é•·åº¦ï¼š{stats['total_content_length']:,} å­—ç¬¦")
    print(f"ğŸ·ï¸  ä¸»é¡Œæ•¸é‡ï¼š{len(stats['topics'])}")
    
    print(f"\nğŸ“‹ å„ä¸»é¡Œè©³ç´°çµ±è¨ˆï¼š")
    print("-" * 60)
    
    for topic, topic_stats in stats["topics"].items():
        print(f"  ğŸ“š {topic}:")
        print(f"    æ–‡æª”æ•¸ï¼š{topic_stats['document_count']}")
        print(f"    è©å½™æ•¸ï¼š{topic_stats['vocab_count']:,}")
        print(f"    å…§å®¹é•·åº¦ï¼š{topic_stats['content_length']:,} å­—ç¬¦")
        print()

def save_statistics(stats: Dict, output_path: str = "vocabulary_import_stats.json"):
    """
    ä¿å­˜çµ±è¨ˆè³‡è¨Šåˆ°æ–‡ä»¶
    
    Args:
        stats (Dict): çµ±è¨ˆè³‡è¨Šå­—å…¸
        output_path (str): è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š çµ±è¨ˆè³‡è¨Šå·²ä¿å­˜åˆ°ï¼š{output_path}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜çµ±è¨ˆè³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

# ============================================================================
# ä¸»ç¨‹å¼é‚è¼¯
# ============================================================================

def main():
    """
    ä¸»ç¨‹å¼å‡½æ•¸
    
    åŸ·è¡Œå®Œæ•´çš„è©å½™å‘é‡åŒ–æµç¨‹ï¼š
    1. è¼‰å…¥è©å½™æ–‡ä»¶
    2. å‰µå»ºå‘é‡è³‡æ–™åº«
    3. é©—è­‰è³‡æ–™åº«å®Œæ•´æ€§
    4. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
    """
    print("VocabVoyage è©å½™å‘é‡è³‡æ–™åº«å»ºç«‹å·¥å…·")
    print("=" * 60)
    
    # æ­¥é©Ÿ 1ï¼šè¼‰å…¥è©å½™æ–‡ä»¶
    print("\nğŸ”„ æ­¥é©Ÿ 1ï¼šè¼‰å…¥è©å½™æ–‡ä»¶")
    print("-" * 30)
    
    documents = load_vocabulary_files()
    
    if not documents:
        print("âŒ æ²’æœ‰è¼‰å…¥åˆ°ä»»ä½•è©å½™æ–‡ä»¶ï¼Œç¨‹å¼çµæŸ")
        return
    
    # æ­¥é©Ÿ 2ï¼šç”Ÿæˆçµ±è¨ˆè³‡è¨Š
    print("\nğŸ”„ æ­¥é©Ÿ 2ï¼šç”Ÿæˆçµ±è¨ˆè³‡è¨Š")
    print("-" * 30)
    
    stats = generate_statistics(documents)
    print_statistics(stats)
    save_statistics(stats)
    
    # æ­¥é©Ÿ 3ï¼šå‰µå»ºå‘é‡è³‡æ–™åº«
    print("\nğŸ”„ æ­¥é©Ÿ 3ï¼šå‰µå»ºå‘é‡è³‡æ–™åº«")
    print("-" * 30)
    
    vectorstore = create_vector_store(documents)
    
    if not vectorstore:
        print("âŒ å‘é‡è³‡æ–™åº«å‰µå»ºå¤±æ•—ï¼Œç¨‹å¼çµæŸ")
        return
    
    # æ­¥é©Ÿ 4ï¼šé©—è­‰è³‡æ–™åº«
    print("\nğŸ”„ æ­¥é©Ÿ 4ï¼šé©—è­‰è³‡æ–™åº«å®Œæ•´æ€§")
    print("-" * 30)
    
    if verify_vector_store(vectorstore):
        print("âœ… è³‡æ–™åº«é©—è­‰æˆåŠŸ")
    else:
        print("âš ï¸  è³‡æ–™åº«é©—è­‰å¤±æ•—ï¼Œä½†è³‡æ–™åº«å·²å‰µå»º")
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ è©å½™å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ è³‡æ–™åº«ä½ç½®ï¼š{CHROMA_DB_DIR}")
    print(f"ğŸ·ï¸  é›†åˆåç¨±ï¼š{COLLECTION_NAME}")
    print(f"ğŸ“Š è™•ç†æ–‡æª”ï¼š{len(documents)} å€‹")
    print(f"ğŸ”¤ ä¼°è¨ˆè©å½™ï¼š{stats['estimated_total_vocab']:,} å€‹")

# ============================================================================
# è¼”åŠ©åŠŸèƒ½
# ============================================================================

def check_prerequisites():
    """æª¢æŸ¥å¿…è¦çš„ä¾è³´å’Œç’°å¢ƒ"""
    print("ğŸ” æª¢æŸ¥ç³»çµ±ç’°å¢ƒ...")
    
    # æª¢æŸ¥ OpenAI API é‡‘é‘°
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸  è­¦å‘Šï¼šæœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹ç¢ºä¿å·²è¨­ç½® OpenAI API é‡‘é‘°")
        return False
    
    # æª¢æŸ¥è©å½™ç›®éŒ„
    if not Path(VOCABULARY_DIR).exists():
        print(f"âŒ éŒ¯èª¤ï¼šè©å½™ç›®éŒ„ä¸å­˜åœ¨ï¼š{VOCABULARY_DIR}")
        print("   è«‹å…ˆé‹è¡Œè©å½™ç”Ÿæˆç¨‹å¼æˆ–æ‰‹å‹•å‰µå»ºè©å½™æ–‡ä»¶")
        return False
    
    print("âœ… ç’°å¢ƒæª¢æŸ¥é€šé")
    return True

def show_help():
    """é¡¯ç¤ºå¹«åŠ©è³‡è¨Š"""
    print("VocabVoyage è©å½™å‘é‡è³‡æ–™åº«å»ºç«‹å·¥å…·")
    print("=" * 60)
    print("\nåŠŸèƒ½èªªæ˜ï¼š")
    print("  é€™å€‹å·¥å…·å°‡è©å½™æ–‡ä»¶è½‰æ›ç‚ºå‘é‡åµŒå…¥ä¸¦å­˜å„²åˆ° Chroma è³‡æ–™åº«ä¸­")
    print("  æ”¯æ´æ‰¹é‡è™•ç†å’Œä¸»é¡Œåˆ†é¡")
    print("\nä½¿ç”¨æ–¹æ³•ï¼š")
    print(f"  python {os.path.basename(__file__)}           # åŸ·è¡Œå®Œæ•´æµç¨‹")
    print(f"  python {os.path.basename(__file__)} --help    # é¡¯ç¤ºå¹«åŠ©")
    print(f"  python {os.path.basename(__file__)} --check   # æª¢æŸ¥ç’°å¢ƒ")
    print("\nå‰ç½®æ¢ä»¶ï¼š")
    print("  1. è¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
    print("  2. åœ¨ data/vocabulary/ ç›®éŒ„ä¸‹æº–å‚™è©å½™æ–‡ä»¶")
    print("  3. å®‰è£å¿…è¦çš„ Python å¥—ä»¶")
    print("\nè¼¸å‡ºï¼š")
    print(f"  - å‘é‡è³‡æ–™åº«ï¼š{CHROMA_DB_DIR}")
    print("  - çµ±è¨ˆå ±å‘Šï¼švocabulary_import_stats.json")

# ============================================================================
# ç¨‹å¼å…¥å£é»
# ============================================================================

if __name__ == "__main__":
    import sys
    
    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        if sys.argv[1] == "--help":
            show_help()
            sys.exit(0)
        elif sys.argv[1] == "--check":
            if check_prerequisites():
                print("âœ… æ‰€æœ‰å‰ç½®æ¢ä»¶éƒ½å·²æ»¿è¶³")
            else:
                print("âŒ è«‹è§£æ±ºä¸Šè¿°å•é¡Œå¾Œå†é‹è¡Œç¨‹å¼")
            sys.exit(0)
    
    # æª¢æŸ¥å‰ç½®æ¢ä»¶
    if not check_prerequisites():
        print("\nè«‹ä½¿ç”¨ --help æŸ¥çœ‹è©³ç´°èªªæ˜")
        sys.exit(1)
    
    # åŸ·è¡Œä¸»ç¨‹å¼
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ ç¨‹å¼åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}")
        import traceback
        traceback.print_exc()